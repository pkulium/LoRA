{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LoRA: Low Rank Adaptation of Large Language Models\n\nOriginal Paper: https://arxiv.org/pdf/2106.09685.pdf","metadata":{}},{"cell_type":"markdown","source":"### Set TPU = False below if using GPU","metadata":{}},{"cell_type":"code","source":"TPU = False\n\nif TPU==True:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\nelse:\n    device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:42:05.035755Z","iopub.execute_input":"2023-11-02T13:42:05.036157Z","iopub.status.idle":"2023-11-02T13:42:05.041352Z","shell.execute_reply.started":"2023-11-02T13:42:05.036116Z","shell.execute_reply":"2023-11-02T13:42:05.039891Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install loralib\n!pip install torchsummary\n!pip install datasets\n!pip install transformers\n!pip install sentencepiece ## For faster tokenization","metadata":{"execution":{"iopub.status.busy":"2023-11-02T13:42:08.971058Z","iopub.execute_input":"2023-11-02T13:42:08.971444Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a992681f550>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/loralib/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a992681f880>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/loralib/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a992681fb20>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/loralib/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7a992681fcd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/loralib/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torchsummary import summary\nimport loralib as lora\nfrom transformers import DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset\nfrom transformers import BertConfig \nfrom transformers import EncoderDecoderConfig \nfrom transformers import DataCollatorWithPadding \nfrom transformers import EncoderDecoderModel \nfrom transformers import AutoTokenizer \nfrom transformers import AutoModelForSequenceClassification \nfrom transformers import CONFIG_MAPPING \nfrom transformers import AutoConfig","metadata":{"id":"5db35d05","execution":{"iopub.status.busy":"2023-06-01T19:40:33.09754Z","iopub.execute_input":"2023-06-01T19:40:33.097995Z","iopub.status.idle":"2023-06-01T19:40:33.106495Z","shell.execute_reply.started":"2023-06-01T19:40:33.097963Z","shell.execute_reply":"2023-06-01T19:40:33.105521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LoRA Functions","metadata":{}},{"cell_type":"code","source":"## Set max matrix rank\nlora_r = 4\n\ndef make_lora_layer(layer, lora_r=4):\n    new_layer = lora.Linear(\n        in_features=layer.in_features,\n        out_features=layer.out_features,\n        bias=layer.bias is None,\n        r=lora_r,\n        merge_weights=False\n    )\n    \n    new_layer.weight = nn.Parameter(layer.weight.detach())\n    \n    if layer.bias is not None:\n        new_layer.bias = nn.Parameter(layer.bias.detach())\n    \n    return new_layer\n\n\ndef make_lora_replace(model, depth=1, path=\"\", verbose=True):\n    if depth > 10:\n        return\n    depth += 1\n        \n    if isinstance(model, nn.Linear) and \"attention\" in path:\n        if verbose:\n            print(f\"Find linear {path}:{key} :\", type(module))\n\n        return make_lora_layer(model)\n    \n    for key in dir(model):\n        module = getattr(model, key)\n        module_type = type(module)\n            \n        if not isinstance(module, nn.Module) or module is model:\n            continue\n\n        if isinstance(module, nn.Linear) and \"attention\" in path:\n            layer = make_lora_layer(module)\n            setattr(model, key, layer)\n            if verbose:\n                print(f\"Find linear {path}:{key} :\", type(module))\n            \n        elif isinstance(module, nn.ModuleList):\n            for i, elem in enumerate(module):\n                layer = make_lora_replace(elem, depth, path+\":\"+key+f\"[{i}]\", verbose=verbose)\n                if layer is not None:\n                    module[i] = layer\n                \n        elif isinstance(module, nn.ModuleDict):\n            for module_key in list(module.keys()):\n                layer = make_lora_replace(item, depth, path+\":\"+key+\":\"+module_key, verbose=verbose)\n                if layer is not None:\n                    module[module_key] = layer\n                \n        else:\n            layer = make_lora_replace(module, depth, path+\":\"+key, verbose=verbose)\n            if layer is not None:\n                setattr(model, key, layer)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:40:33.107941Z","iopub.execute_input":"2023-06-01T19:40:33.108277Z","iopub.status.idle":"2023-06-01T19:40:33.124927Z","shell.execute_reply.started":"2023-06-01T19:40:33.108251Z","shell.execute_reply":"2023-06-01T19:40:33.123998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fetch Data, Models","metadata":{}},{"cell_type":"code","source":"dataset_id, task, tok_train_fold, sentence1_key, num_labels, vocab_len = 'tweet_eval', 'emotion', 'train', 'text', 4, 30000\n\ndataset = load_dataset(dataset_id, task)\n\nUSE_LORA = True\nmodel_name = 'roberta-large'\nbatch_size = 16\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ncollator = DataCollatorWithPadding(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:40:33.127106Z","iopub.execute_input":"2023-06-01T19:40:33.127477Z","iopub.status.idle":"2023-06-01T19:40:38.277349Z","shell.execute_reply.started":"2023-06-01T19:40:33.127445Z","shell.execute_reply":"2023-06-01T19:40:38.276128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_LORA:\n#     first_output = model(**collator([tokenizer('test')]))\n\n    make_lora_replace(model, verbose=True)\n\n#     final_output = model(**collator([tokenizer('test')]))","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:40:38.279421Z","iopub.execute_input":"2023-06-01T19:40:38.279738Z","iopub.status.idle":"2023-06-01T19:40:41.366962Z","shell.execute_reply.started":"2023-06-01T19:40:38.279711Z","shell.execute_reply":"2023-06-01T19:40:41.365755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load omdel onto TPU/GPU\nmodel = model.to(device)\n# tokenizer = tokenizer.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:40:41.368124Z","iopub.execute_input":"2023-06-01T19:40:41.368419Z","iopub.status.idle":"2023-06-01T19:40:41.802046Z","shell.execute_reply.started":"2023-06-01T19:40:41.368394Z","shell.execute_reply":"2023-06-01T19:40:41.800754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply LoRA to model ","metadata":{}},{"cell_type":"code","source":"lora_r = 4 ## Try different max matrix ranks for different results\n\ntotal_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters before LoRA: {total_trainable_params}\")\n\n## Apply LoRA\nlora.mark_only_lora_as_trainable(model)\n\ntotal_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters after LoRA: {total_trainable_params}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:40:41.803343Z","iopub.execute_input":"2023-06-01T19:40:41.80369Z","iopub.status.idle":"2023-06-01T19:40:41.821215Z","shell.execute_reply.started":"2023-06-01T19:40:41.80364Z","shell.execute_reply":"2023-06-01T19:40:41.820344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if \"deberta\" not in name:\n        print(name)\n#             print(param.shape)\n        param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:36:02.520179Z","iopub.execute_input":"2023-06-01T19:36:02.520585Z","iopub.status.idle":"2023-06-01T19:36:02.544401Z","shell.execute_reply.started":"2023-06-01T19:36:02.52055Z","shell.execute_reply":"2023-06-01T19:36:02.54318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import wandb\n\n# wandb.init(project='Lora')","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:36:02.545884Z","iopub.execute_input":"2023-06-01T19:36:02.546533Z","iopub.status.idle":"2023-06-01T19:36:02.554659Z","shell.execute_reply.started":"2023-06-01T19:36:02.546496Z","shell.execute_reply":"2023-06-01T19:36:02.553746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process Training Data","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[sentence1_key], truncation=True, padding=True)\n\nencoded_dataset = dataset.map(preprocess_function, batched=True)\n\nencoded_dataset = encoded_dataset.remove_columns(['text'])\n\ndataloaders = {\n    key: DataLoader(ds, shuffle=True, collate_fn=collator, num_workers=2, batch_size=batch_size) for key, ds in encoded_dataset.items()\n}","metadata":{"execution":{"iopub.status.busy":"2023-06-01T18:54:04.206664Z","iopub.execute_input":"2023-06-01T18:54:04.206953Z","iopub.status.idle":"2023-06-01T18:54:04.449107Z","shell.execute_reply.started":"2023-06-01T18:54:04.206928Z","shell.execute_reply":"2023-06-01T18:54:04.447991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Batching Helpers","metadata":{}},{"cell_type":"code","source":"def predict_loader(dataloader):\n    model.eval()\n    res = []\n    labels = []\n    \n    for batch in tqdm(dataloader):\n        batch = batch_device(batch)\n        output = model(**batch).logits.argmax(dim=-1).detach().cpu().numpy()\n        res.extend(output)\n        labels.extend(batch.labels.detach().cpu().numpy())\n            \n    return(res, labels)\n\n\ndef batch_device(batch):\n    for key in list(batch.keys()):\n        batch[key] = batch[key].to(device)\n        \n    return(batch)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T18:54:04.450341Z","iopub.execute_input":"2023-06-01T18:54:04.450623Z","iopub.status.idle":"2023-06-01T18:54:04.458355Z","shell.execute_reply.started":"2023-06-01T18:54:04.450599Z","shell.execute_reply":"2023-06-01T18:54:04.457474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.watch(model)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T18:54:04.459399Z","iopub.execute_input":"2023-06-01T18:54:04.459686Z","iopub.status.idle":"2023-06-01T18:54:04.478716Z","shell.execute_reply.started":"2023-06-01T18:54:04.459649Z","shell.execute_reply":"2023-06-01T18:54:04.477749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\nfrom tqdm.autonotebook import tqdm\nfrom sklearn.metrics import f1_score\n\nlr = 2e-5\nif USE_LORA:\n    lr = 2e-4\n\noptimizer = Adam([param for param in model.parameters() if param.requires_grad], lr=lr)\nsteps = len(dataloaders['train'])\nepochs = 15\nscheduler = get_cosine_schedule_with_warmup(optimizer, steps * 1, steps * epochs)\nbest_f1 = 0\n\n\nfor i in range(epochs):\n    model.train()\n    losses = []\n    for batch in tqdm(dataloaders['train']):\n        batch = batch_device(batch)\n        \n        optimizer.zero_grad()\n        output = model(**batch)\n\n        output.loss.backward()\n#         wandb.log({\n#             \"train/loss\": output.loss.detach().cpu().numpy()\n#         })\n        \n        optimizer.step()\n        scheduler.step()\n    \n    res, labels = predict_loader(dataloaders['validation'])\n    res = np.array(res) \n    labels = np.array(labels)\n    f1 = f1_score(labels, res, average='micro')\n#     wandb.log({\n#         \"eval/f1\": f1\n#     })\n#     print(f1)\n    \n    if f1 > best_f1:\n        best_f1 = f1\n        checkpoint_path = \"best_lora_checkpoint.pth\"\n        \n        if USE_LORA:\n            torch.save(lora.lora_state_dict(model), checkpoint_path)\n        else:\n            torch.save(model.state_dict(), checkpoint_path)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-01T18:54:04.479876Z","iopub.execute_input":"2023-06-01T18:54:04.480247Z","iopub.status.idle":"2023-06-01T18:59:20.245901Z","shell.execute_reply.started":"2023-06-01T18:54:04.480222Z","shell.execute_reply":"2023-06-01T18:59:20.244054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}